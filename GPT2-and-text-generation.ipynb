{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"GPT2-and-text-generation.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MlU_ENH5du32","colab_type":"text"},"source":["# 1 字节对编码"]},{"cell_type":"code","metadata":{"id":"eGOrgCe4d6Bk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917217094,"user_tz":-480,"elapsed":2729,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"7891db6c-8d1f-4849-ebde-22635d981f2b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XsvnELRtfCIw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917217095,"user_tz":-480,"elapsed":2720,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"54ecb4ef-191c-4378-f822-9504beb5731d"},"source":["pwd"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"1bfKwuvdfC_g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917217096,"user_tz":-480,"elapsed":2709,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"275b5d5f-35a2-498f-ffcc-07d11711d6be"},"source":["cd /content/drive/My Drive/my_code/GPT2/GPT2-and-text-generation"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/my_code/GPT2/GPT2-and-text-generation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cLA5F9AkfNf_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1599917218033,"user_tz":-480,"elapsed":3635,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"d65e7304-6521-44c1-ac0d-069b31c987f6"},"source":["ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["config.json                     pytorch_model.bin\n","GPT2-and-text-generation.ipynb  romeo_and_juliet.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9alxfWoodu34","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917218034,"user_tz":-480,"elapsed":3627,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["import re\n","import collections\n","\n","\n","def get_stats(vocab):\n","    pairs = collections.defaultdict(int)\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i in range(len(symbols)-1):\n","            pairs[symbols[i], symbols[i+1]] += freq  # 计算字节对出现频率\n","    return pairs\n","\n","\n","def merge_vocab(pair, v_in):\n","    v_out = {}\n","    bigram = re.escape(' '.join(pair))  # 将字节对中可解释为正则运算符的字符转义\n","    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')  # 将要合并的字节对前后只能为空白字符\n","    for word in v_in:\n","        w_out = p.sub(''.join(pair), word)  # 合并符合条件的字节对\n","        v_out[w_out] = v_in[word]\n","    return v_out"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"0J67GOuIdu37","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917218034,"user_tz":-480,"elapsed":3621,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,\n","         'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIiOP3pHdu3_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"ok","timestamp":1599917218035,"user_tz":-480,"elapsed":3615,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"c367f91a-4104-4297-f6d1-1efc2a813e09"},"source":["get_stats(vocab)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(int,\n","            {('d', 'e'): 3,\n","             ('e', 'r'): 2,\n","             ('e', 's'): 9,\n","             ('e', 'w'): 6,\n","             ('i', 'd'): 3,\n","             ('l', 'o'): 7,\n","             ('n', 'e'): 6,\n","             ('o', 'w'): 7,\n","             ('r', '</w>'): 2,\n","             ('s', 't'): 9,\n","             ('t', '</w>'): 9,\n","             ('w', '</w>'): 5,\n","             ('w', 'e'): 8,\n","             ('w', 'i'): 3})"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"04BbRz70du4D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1599917218036,"user_tz":-480,"elapsed":3607,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"f90aa461-7fbd-456b-ea47-8f5795f7b21c"},"source":["num_merges = 10\n","for i in range(num_merges):\n","    pairs = get_stats(vocab)\n","    best = max(pairs, key=pairs.get)  # 选择频率最大的字节对\n","    vocab = merge_vocab(best, vocab)\n","    print(best)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["('e', 's')\n","('es', 't')\n","('est', '</w>')\n","('l', 'o')\n","('lo', 'w')\n","('n', 'e')\n","('ne', 'w')\n","('new', 'est</w>')\n","('low', '</w>')\n","('w', 'i')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dThooSuddu4F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917218036,"user_tz":-480,"elapsed":3599,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"a708a390-c880-4041-a92b-bd4ce59d4d3c"},"source":["vocab"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'low e r </w>': 2, 'low</w>': 5, 'newest</w>': 6, 'wi d est</w>': 3}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"blgB034jdu4I","colab_type":"text"},"source":["# 2 top-k 实现"]},{"cell_type":"code","metadata":{"id":"PLxHkRb1du4J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917218037,"user_tz":-480,"elapsed":3591,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["import random\n","\n","def select_top_k(predictions, k=10):\n","    predicted_index = random.choice(\n","        predictions[0, -1, :].sort(descending=True)[1][:10]).item()\n","    return predicted_index"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0IPj3iiudu4M","colab_type":"text"},"source":["# 3 预训练模型生成新闻"]},{"cell_type":"code","metadata":{"id":"gbwHOF6Adu4M","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917218038,"user_tz":-480,"elapsed":3584,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["import torch\n","from pytorch_transformers import GPT2Tokenizer\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"043vBPKydu4R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"status":"ok","timestamp":1599917218039,"user_tz":-480,"elapsed":3576,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"9754ca49-db10-440f-eb0c-b9c7f840ebf8"},"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /tmp/tmprrs3urgt\n","100%|██████████| 1042301/1042301 [00:00<00:00, 7066834.01B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmprrs3urgt to cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmprrs3urgt\n","INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /tmp/tmpaaib9bsy\n","100%|██████████| 456318/456318 [00:00<00:00, 3725380.17B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmpaaib9bsy to cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpaaib9bsy\n","INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lrWA5YGtdu4T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917218039,"user_tz":-480,"elapsed":3568,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"19238bc4-0b3f-4622-f08e-47f99b00e68c"},"source":["text = \"guobing, you are so fat,\"\n","indexed_tokens = tokenizer.encode(text)\n","indexed_tokens"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[915, 672, 278, 11, 345, 389, 523, 3735, 11]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"Ndv_kF99du4W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917218040,"user_tz":-480,"elapsed":3562,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"eb66be35-b1e6-42b8-a257-6115509dafe7"},"source":["tokens_tensor = torch.tensor([indexed_tokens])\n","tokens_tensor.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 9])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"dowHDAc3du4a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"status":"ok","timestamp":1599917229380,"user_tz":-480,"elapsed":14895,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"326709c2-3e90-469e-a173-267edfb8c3e6"},"source":["from pytorch_transformers import GPT2LMHeadModel\n","\n","# 读取 GPT-2 预训练模型\n","model = GPT2LMHeadModel.from_pretrained(\"./\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["INFO:pytorch_transformers.modeling_utils:loading configuration file ./config.json\n","INFO:pytorch_transformers.modeling_utils:Model config {\n","  \"attn_pdrop\": 0.1,\n","  \"embd_pdrop\": 0.1,\n","  \"finetuning_task\": null,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"num_labels\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pruned_heads\": {},\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"torchscript\": false,\n","  \"vocab_size\": 50257\n","}\n","\n","INFO:pytorch_transformers.modeling_utils:loading weights file ./pytorch_model.bin\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VzHV5m-0du4d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599917229382,"user_tz":-480,"elapsed":14891,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"8cfefb52-7374-418b-a6db-8abf5a411c40"},"source":["model.eval()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"b3VGsZT0du4g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917229383,"user_tz":-480,"elapsed":14885,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"4bd6f862-b850-4be5-a6f1-956cfab29617"},"source":["tokens_tensor.shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 9])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"4hzxsOp_du4j","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917229384,"user_tz":-480,"elapsed":14879,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["text = \"guobing, you are so fat,\""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPh4IJKTdu4l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1599917264314,"user_tz":-480,"elapsed":49802,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"2b5501ae-e247-45f2-fbba-8d32d567aa99"},"source":["total_predicted_text = text\n","n = 100  # 预测过程的循环次数\n","for _ in range(n):\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["guobing, you are so fat, that it takes a bit. So, I am very lucky that this year has had this big and wonderful opportunity, which means so I can take the risk with you.\" The first few hours on the bike are hard and hard but the second is just too long so it doesn`nt feel like there has even been anything on. \"When a big mountain is coming to a place, we need to go. We don `d go out with no bikes at the top, we just want it there\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6X1_7NhUdu4n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917283221,"user_tz":-480,"elapsed":3006,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"7d5c0b65-8b5f-4543-a4af-3606c33eafe6"},"source":["with open('./romeo_and_juliet.txt', 'r') as f:\n","    dataset = f.read()\n","\n","len(dataset)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["138150"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"hPirjoACdu4q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1599917293941,"user_tz":-480,"elapsed":3130,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"bcf9f102-a2dc-4454-9662-a49e6cc01bd9"},"source":["indexed_text = tokenizer.encode(dataset)\n","del(dataset)\n","\n","dataset_cut = []\n","for i in range(len(indexed_text)//512):\n","    # 将字符串分段成长度为 512\n","    dataset_cut.append(indexed_text[i*512:i*512+512])\n","del(indexed_text)\n","\n","dataset_tensor = torch.tensor(dataset_cut)\n","dataset_tensor.shape"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:pytorch_transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (41669 > 1024). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([81, 512])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"1d8ODo1lgG4n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917303790,"user_tz":-480,"elapsed":3096,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"b4c20793-7f32-4dc6-987d-a4b3c5109561"},"source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","# 构建数据集和数据迭代器，设定 batch_size 大小为 2\n","train_set = TensorDataset(dataset_tensor,\n","                          dataset_tensor)  # 标签与样本数据相同\n","train_loader = DataLoader(dataset=train_set,\n","                          batch_size=2,\n","                          shuffle=False)\n","train_loader"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7fbffb389e10>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"ASH_Mo7EgJom","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599917311028,"user_tz":-480,"elapsed":2677,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"4166ae07-9ee2-435f-e8e7-a176ca6dd2ba"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"v8ruJHXbgLl3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":575},"executionInfo":{"status":"ok","timestamp":1599917914533,"user_tz":-480,"elapsed":596717,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"36eb9c56-8c5a-4350-cc54-57fc35bf7eff"},"source":["from torch import nn\n","from torch.autograd import Variable\n","import time\n","\n","pre = time.time()\n","\n","epoch = 30  # 循环学习 30 次\n","\n","model.to(device)\n","model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # 定义优化器\n","\n","for i in range(epoch):\n","    total_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data).to(device), Variable(\n","            target).to(device)\n","\n","        optimizer.zero_grad()\n","\n","        loss, logits, _ = model(data, labels=target)\n","\n","        total_loss += loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx == len(train_loader)-1:\n","            # 在每个 Epoch 的最后输出一下结果\n","            print('average loss:', total_loss/len(train_loader))\n","\n","print('训练时间：', time.time()-pre)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["average loss: tensor(4.0316, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.8470, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.7315, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.6476, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.5738, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.5079, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.4507, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.3989, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.3539, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.3105, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.2676, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.2294, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.1954, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.1571, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.1239, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.0921, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.0552, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(3.0249, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.9919, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.9639, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.9315, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.8956, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.8656, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.8346, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.8018, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.7733, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.7450, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.7150, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.6742, device='cuda:0', grad_fn=<DivBackward0>)\n","average loss: tensor(2.6510, device='cuda:0', grad_fn=<DivBackward0>)\n","训练时间： 594.805577993393\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IRo8Mvk7gN5_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599917914534,"user_tz":-480,"elapsed":582601,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}}},"source":["text = \"From fairest creatures we desire\"  # 这里也可以输入不同的英文文本\n","indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD6DtBYigRWd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"status":"ok","timestamp":1599918107894,"user_tz":-480,"elapsed":19784,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"e9abb40f-45d4-429d-892d-bc5be3b0037d"},"source":["model.eval()\n","total_predicted_text = text\n","\n","# 使训练后的模型进行 500 次预测\n","for _ in range(500):\n","    tokens_tensor = tokens_tensor.to('cuda')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","\n","    if len(indexed_tokens) > 1023:\n","        # 模型最长输入长度为1024，如果长度过长则截断\n","        indexed_tokens = indexed_tokens[-1023:]\n","\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["From fairest creatures we desire: let it, and it not stand alone:\n","And if it please the eyes of mortals, we have but the bare\n","wood that they can use. Thus I'll set up our\n","assembly; for this place lies so high above all other encampment in Tyne\n","That if men do wrong and trespass, it may afford remedy, and death: hence we encamp!\n","ROMAN'S LA\n","I'll not leave you here in thy bed: I'll bring thy sword and arrowhead; they're not yet\n","done, but you may be well, as thou livest, to fight for these murders: hence forth thou art done with thimble\n","to-dower; the poison I gave my master in this desperate state is to kill thyself; thou shalt have no need for him:\n","But go along hence; I must bring him back to Tyche. He will die young: for that I am,\n","you must take it from the morgol. Romeo will have thy blade with her. What? I have not kill thee yet, for that will bring thee\n","some pleasure. O God I love you--love--love!--you have slain this tyrant and Romeo that thou hast slain with\n","such fury and murder that Romeo should not love her:--I love, God grant you pardon\n","I never intended for this. Verse, hence to Juliet's. JUL\n","Go and hide,--here she stands: she hath hid her father:\n","Her son comes hither, he and his daughter\n","shall die together at this night\n","and be Romeo. Romeo lives: be careful. Go away from thence, for I must return hither. Romeo lives, Juliet--Go away from hither, for that shall bring hither death:\n","And go along alone! thou can I but weep, for I shall never\n","have a husband, nor a sister; therefore let's leave thy kins' lives alone\n","here in Tycho; or rather go and seek comfort\n","with Juliet's kindred--O blessed child, that heaven dost make her so, O! behold--a son and his lady--A true daughter; Romeo is born: marry him\n","now that Juliet can bear her again!\n","I'll to Mantinea to-dame: he dar my lady hath been gone for some years: I shall bring her here:--I\n","wol my lady, good lord, thou should'sst marry this poor woman of old--God\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9IN3QXFVjJ9X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1599918169912,"user_tz":-480,"elapsed":23029,"user":{"displayName":"GBing L","photoUrl":"","userId":"09797849703380162798"}},"outputId":"378ba7f0-347b-4419-ff78-25d367e62747"},"source":["text = \"guobing, you are so fat,\"\n","indexed_tokens = tokenizer.encode(text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","model.eval()\n","total_predicted_text = text\n","\n","# 使训练后的模型进行 500 次预测\n","for _ in range(500):\n","    tokens_tensor = tokens_tensor.to('cuda')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor)\n","        predictions = outputs[0]\n","\n","    predicted_index = select_top_k(predictions, k=10)\n","\n","    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","    total_predicted_text += tokenizer.decode(predicted_index)\n","    if '<|endoftext|>' in total_predicted_text:\n","        # 如果出现文本结束标志，就结束文本生成\n","        break\n","\n","    indexed_tokens += [predicted_index]\n","\n","    if len(indexed_tokens) > 1023:\n","        # 模型最长输入长度为1024，如果长度过长则截断\n","        indexed_tokens = indexed_tokens[-1023:]\n","\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","\n","print(total_predicted_text)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["guobing, you are so fat, O me. Come now to my chamber; where is Romeo to lie! O Juliet--Hush; she will give thee her rest: go to Montag:\n","O Montignac; there lies Romeo. O mad Juliet, what art's her here--\n","Where art these poor little friars?\n","'Alas!' quears the friAR! I see you must have an eye, O Romeo; I\n","see that Juliet bears such fruit, O mad Paris!' Come! what an ill humour this Romeo\n","wakes you! Come thou now hither: come I now, nurse--\n","O, my head is heavy--Come now; come hither again:' Come thou again: come, go! '\n","CAPulete cries out again--CAPULEt\n","Where have the Montserrel come to die? come now again, and stay: Juliet, I must kill myself\n","--I'll, kill you all--but, my heart--Go. Juliet rushes in: Juliet cries\n","And comes, O good nurse. 'Come now! my heart!' and 'O happy day is upon her; go to her. I see--she\n","will look after thee,--\n","Herself, my love--'Ah! what is her name again'?--Hail me, Romeo!'\n","O Juliet,' say ye: I am so ill-wisher--'Al as ill! O my head--I must die!' I am gone--I come again. What is Romeo--Ah, mad Montagueso!--O Juliet,\n","My Romeo's man--What an aeeling wag thou shalt see: Romeo!' 'Come now: look thou back:' and I come! Juliet comes: Ah!' Ah,' what! Romeo is in his shroud!--\n","I say--here he be: come forth and kill the man: O! what, my son!'\n","CAPUMEAUUE falls\n","Nurse? what a wretch are yoked in!\n","JUNE JULIS is in heaven,--Nursed,\n","Hence the earth tremans and throws forth her fiery wings like lightning\n","To pounce forth her holy fiery wheels to rip down our\n","saintful and delectated idol. I am Romeo!' What dut'er art thou there\n","Where I once saw so fair,\n","That with thy help she may make thee an everlasting saint? O! my child--I have known this child since childhood, my\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BjjmKXL9jYTu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}